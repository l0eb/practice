{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aim: Linear Regression by using Deep Neural Network\n",
    "In [ ]:\n",
    "In [1]:\n",
    "In [2]:\n",
    "In [3]:\n",
    "In [4]:\n",
    "In [5]:\n",
    "In [6]:\n",
    "In [7]:\n",
    "In [8]:\n",
    "(404, 13)\n",
    "(102, 13)\n",
    "(404,)\n",
    "(102,)\n",
    "Out[4]: array([ 1.23247, 0. , 8.14 , 0. , 0.538 , 6.142 ,\n",
    " 91.7 , 3.9769 , 4. , 307. , 21. , 396.9 ,\n",
    " 18.72 ])\n",
    "Out[5]: 15.2\n",
    "Out[7]: array([0.0024119 , 0. , 0.01592969, 0. , 0.00105285,\n",
    " 0.01201967, 0.17945359, 0.00778265, 0.00782786, 0.6007879 ,\n",
    " 0.04109624, 0.77671895, 0.03663436])\n",
    "Out[8]: 15.2\n",
    "pip install tensorflow --user --no-warn-script-location\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "from sklearn import preprocessing\n",
    "(train_x, train_y),(test_x,test_y)=boston_housing.load_data()\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)\n",
    "train_x[0]\n",
    "train_y[0]\n",
    "train_x=preprocessing.normalize(train_x)\n",
    "test_x=preprocessing.normalize(test_x)\n",
    "train_x[0]\n",
    "train_y[0]\n",
    "In [9]:\n",
    "In [10]:\n",
    "In [11]:\n",
    "In [12]:\n",
    "Epoch 1/100\n",
    "404/404 [==============================] - 1s 2ms/step - loss: 145.2648 -\n",
    "mae: 8.8432 - val_loss: 70.4067 - val_mae: 6.0675\n",
    "Epoch 2/100\n",
    "404/404 [==============================] - 2s 4ms/step - loss: 69.6452 -\n",
    "mae: 5.8083 - val_loss: 60.2890 - val_mae: 5.7936\n",
    "Epoch 3/100\n",
    "404/404 [==============================] - 2s 4ms/step - loss: 64.0329 -\n",
    "mae: 5.4785 - val_loss: 58.2462 - val_mae: 5.5318\n",
    "Epoch 4/100\n",
    "404/404 [==============================] - 2s 4ms/step - loss: 62.4070 -\n",
    "mae: 5.4575 - val_loss: 55.6510 - val_mae: 5.6276\n",
    "Epoch 5/100\n",
    "404/404 [==============================] - 1s 4ms/step - loss: 57.9584 -\n",
    "mae: 5.3383 - val_loss: 54.0462 - val_mae: 5.4752\n",
    "Epoch 6/100\n",
    "404/404 [==============================] - 1s 3ms/step - loss: 58.4756 -\n",
    "mae: 5.2441 - val_loss: 61.6325 - val_mae: 5.5175\n",
    "Epoch 7/100\n",
    "404/404 [ ] 2s 4ms/step loss: 55 2975\n",
    "Actual output : 15.2\n",
    "1/1 [==============================] - 0s 105ms/step\n",
    "Predicted output: [[15.099294]]\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "def HPPM():\n",
    " model=Sequential()\n",
    " model.add(Dense(128, activation='relu', input_shape=(train_x[0].shape)))\n",
    " model.add(Dense(64, activation='relu'))\n",
    " model.add(Dense(32, activation='relu'))\n",
    " model.add(Dense(1))\n",
    " model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    " return model\n",
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples=len(train_x)\n",
    "num_epochs=100\n",
    "all_scores=[]\n",
    "model=HPPM()\n",
    "history=model.fit(x=train_x, y=train_y, epochs=num_epochs, batch_size=1, verbo\n",
    "test_input=[(8.65407330e-05, 0.00000000e+00, 1.13392175e-02, 0.00000000e+00, 1\n",
    "print(\"Actual output : 15.2\")\n",
    "print(\"Predicted output: \", model.predict(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aim: Classification using Deep Neural Network\n",
    "In [1]:\n",
    "In [2]:\n",
    "In [3]:\n",
    "In [4]:\n",
    "In [5]:\n",
    "In [6]:\n",
    "Review is [1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, 2, 32, 8\n",
    "5, 156, 45, 40, 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 2, 16, 3\n",
    "804, 8, 4, 226, 65, 12, 43, 127, 24, 2, 10, 10]\n",
    "Review is 0\n",
    "{'fawn': 34701, 'tsukino': 52006, 'nunnery': 52007, 'sonja': 16816, 'van\n",
    "i': 63951, 'woods': 1408, 'spiders': 16115, 'hanging': 2345, 'woody': 228\n",
    "9, 'trawling': 52008, \"hold's\": 52009, 'comically': 11307, 'localized': 4\n",
    "0830, 'disobeying': 30568, \"'royale\": 52010, \"harpo's\": 40831, 'canet': 5\n",
    "2011, 'aileen': 19313, 'acurately': 52012, \"diplomat's\": 52013, 'rickma\n",
    "n': 25242, 'arranged': 6746, 'rumbustious': 52014, 'familiarness': 52015,\n",
    "\"spider'\": 52016, 'hahahah': 68804, \"wood'\": 52017, 'transvestism': 4083\n",
    "3, \"hangin'\": 34702, 'bringing': 2338, 'seamier': 40834, 'wooded': 34703,\n",
    "'bravora': 52018, 'grueling': 16817, 'wooden': 1636, 'wednesday': 16818,\n",
    "\"'prix\": 52019, 'altagracia': 34704, 'circuitry': 52020, 'crotch': 11585,\n",
    "'busybody': 57766, \"tart'n'tangy\": 52021, 'burgade': 14129, 'thrace': 520\n",
    "23, \"tom's\": 11038, 'snuggles': 52025, 'francesco': 29114, 'complainers':\n",
    "52027, 'templarios': 52125, '272': 40835, '273': 52028, 'zaniacs': 52130,\n",
    "'275': 34706, 'consenting': 27631, 'snuggled': 40836, 'inanimate': 15492,\n",
    "'uality': 52030, 'bronte': 11926, 'errors': 4010, 'dialogs': 3230, \"yomad\n",
    "a's\": 52031, \"madman's\": 34707, 'dialoge': 30585, 'usenet': 52033, 'video\n",
    "drome': 40837, \"kid'\": 26338, 'pawed': 52034, \"'girlfriend'\": 30569, \"'pl\n",
    "easure\": 52035, \"'reloaded'\": 52036, \"kazakos'\": 40839, 'rocque': 52037,\n",
    "'mailings': 52038, 'brainwashed': 11927, 'mcanally': 16819, \"tom''\": 5203\n",
    "9 'k t' 25243 ' ffili t d' 21905 'b b h' 52040 \" ' \" 4\n",
    "Out[5]: (25000,)\n",
    "Out[6]: (25000,)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import imdb\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000) \n",
    "data = np.concatenate((X_train, X_test), axis=0)\n",
    "label = np.concatenate((y_train, y_test), axis=0)\n",
    "print(\"Review is \",X_train[5]) \n",
    "print(\"Review is \",y_train[5])\n",
    "vocab=imdb.get_word_index() \n",
    "print(vocab) \n",
    "X_train.shape\n",
    "X_test.shape\n",
    "In [7]:\n",
    "In [8]:\n",
    "In [9]:\n",
    "In [10]:\n",
    "In [11]:\n",
    "In [12]:\n",
    "In [13]:\n",
    "Out[7]: array([1, 0, 0, ..., 0, 1, 0], dtype=int64)\n",
    "Out[8]: array([0, 1, 1, ..., 0, 0, 0], dtype=int64)\n",
    "Out[11]: array([1, 0, 0, ..., 1, 0, 0], dtype=int64)\n",
    "Label: 1\n",
    "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 3\n",
    "6, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 17\n",
    "2, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 1\n",
    "92, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12,\n",
    "16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2,\n",
    "5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4,\n",
    "130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12,\n",
    "215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2,\n",
    "7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13,\n",
    "104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 22\n",
    "6, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25,\n",
    "104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 1\n",
    "5, 16, 5345, 19, 178, 32]\n",
    "y_train\n",
    "y_test\n",
    "def vectorize(sequences, dimension = 10000):\n",
    " # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    " results = np.zeros((len(sequences), dimension))\n",
    " for i, sequence in enumerate(sequences):\n",
    " results[i, sequence] = 1\n",
    " return results\n",
    "test_x = data[:10000]\n",
    "test_y = label[:10000]\n",
    "train_x = data[10000:]\n",
    "train_y = label[10000:]\n",
    "test_y\n",
    "print(\"Label:\", label[0])\n",
    "print(data[0])\n",
    "In [14]:\n",
    "In [15]:\n",
    "# this film was just brilliant casting location scenery story direction ever\n",
    "yone's really suited the part they played and you could just imagine being t\n",
    "here robert # is an amazing actor and now the same being director # father c\n",
    "ame from the same scottish island as myself so i loved the fact there was a\n",
    "real connection with this film the witty remarks throughout the film were gr\n",
    "eat it was just brilliant so much that i bought the film as soon as it was r\n",
    "eleased for # and would recommend it to everyone to watch and the fly fishin\n",
    "g was amazing really cried at the end it was so sad and you know what they s\n",
    "ay if you cry at a film it must have been good and this definitely was also\n",
    "# to the two little boy's that played the # of norman and paul they were jus\n",
    "t brilliant children are often left out of the # list i think because the st\n",
    "ars that play them all grown up are such a big profile for the whole film bu\n",
    "t these children are amazing and should be praised for what they have done d\n",
    "on't you think the whole story was so lovely because it was true and was som\n",
    "eone's life after all that was shared with us all\n",
    "Out[15]: {'fawn': 34701,\n",
    "'tsukino': 52006,\n",
    "'nunnery': 52007,\n",
    "'sonja': 16816,\n",
    "'vani': 63951,\n",
    "'woods': 1408,\n",
    "'spiders': 16115,\n",
    "'hanging': 2345,\n",
    "'woody': 2289,\n",
    "'trawling': 52008,\n",
    "\"hold's\": 52009,\n",
    "'comically': 11307,\n",
    "'localized': 40830,\n",
    "'disobeying': 30568,\n",
    "\"'royale\": 52010,\n",
    "\"harpo's\": 40831,\n",
    "'canet': 52011,\n",
    "'aileen': 19313,\n",
    "'acurately': 52012,\n",
    "\"diplomat's\": 52013\n",
    "index = imdb.get_word_index()\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
    "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[0]] ) \n",
    "print(decoded) \n",
    "index\n",
    "In [16]:\n",
    "In [17]:\n",
    "In [18]:\n",
    "In [19]:\n",
    "In [20]:\n",
    "Out[16]: {34701: 'fawn',\n",
    "52006: 'tsukino',\n",
    "52007: 'nunnery',\n",
    "16816: 'sonja',\n",
    "63951: 'vani',\n",
    "1408: 'woods',\n",
    "16115: 'spiders',\n",
    "2345: 'hanging',\n",
    "2289: 'woody',\n",
    "52008: 'trawling',\n",
    "52009: \"hold's\",\n",
    "11307: 'comically',\n",
    "40830: 'localized',\n",
    "30568: 'disobeying',\n",
    "52010: \"'royale\",\n",
    "40831: \"harpo's\",\n",
    "52011: 'canet',\n",
    "19313: 'aileen',\n",
    "52012: 'acurately',\n",
    "52013 \"di l t' \"\n",
    "Out[17]: \"# this film was just brilliant casting location scenery story direction eve\n",
    "ryone's really suited the part they played and you could just imagine being\n",
    "there robert # is an amazing actor and now the same being director # father\n",
    "came from the same scottish island as myself so i loved the fact there was a\n",
    "real connection with this film the witty remarks throughout the film were gr\n",
    "eat it was just brilliant so much that i bought the film as soon as it was r\n",
    "eleased for # and would recommend it to everyone to watch and the fly fishin\n",
    "g was amazing really cried at the end it was so sad and you know what they s\n",
    "ay if you cry at a film it must have been good and this definitely was also\n",
    "# to the two little boy's that played the # of norman and paul they were jus\n",
    "t brilliant children are often left out of the # list i think because the st\n",
    "ars that play them all grown up are such a big profile for the whole film bu\n",
    "t these children are amazing and should be praised for what they have done d\n",
    "on't you think the whole story was so lovely because it was true and was som\n",
    "eone's life after all that was shared with us all\"\n",
    "Out[20]: (40000, 10000)\n",
    "reverse_index\n",
    "decoded\n",
    "data = vectorize(data)\n",
    "label = np.array(label).astype(\"float32\")\n",
    "# Creating train and test data set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,label, test_size=0.20\n",
    "X_train.shape\n",
    "In [21]:\n",
    "In [22]:\n",
    "In [23]:\n",
    "In [24]:\n",
    "Out[21]: (10000, 10000)\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type) Output Shape Param # \n",
    "=================================================================\n",
    "dense (Dense) (None, 50) 500050 \n",
    " \n",
    "dropout (Dropout) (None, 50) 0 \n",
    "dense_1 (Dense) (None, 50) 2550 \n",
    "dropout_1 (Dropout) (None, 50) 0 \n",
    "dense_2 (Dense) (None, 50) 2550 \n",
    "dense_3 (Dense) (None, 1) 51 \n",
    "=================================================================\n",
    "Total params: 505,201\n",
    "Trainable params: 505,201\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "X_test.shape\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "# Input - Layer\n",
    "model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000, )))\n",
    "# Hidden - Layers\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\")) #ReLU\" stands for Rectified L\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\")) #adds another Dense layer t\n",
    "model.summary()\n",
    "import tensorflow as tf\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "In [25]:\n",
    "In [26]:\n",
    "In [27]:\n",
    "In [28]:\n",
    "Epoch 1/2\n",
    "80/80 [==============================] - 5s 46ms/step - loss: 0.4119 - accur\n",
    "acy: 0.8105 - val_loss: 0.2616 - val_accuracy: 0.8968\n",
    "Epoch 2/2\n",
    "80/80 [==============================] - 3s 38ms/step - loss: 0.2191 - accur\n",
    "acy: 0.9158 - val_loss: 0.2548 - val_accuracy: 0.8989\n",
    "0.897849977016449\n",
    "313/313 [==============================] - 1s 3ms/step\n",
    "Out[28]: array([[0.1319962 ],\n",
    " [0.9938973 ],\n",
    " [0.8705446 ],\n",
    " ...,\n",
    " [0.95068085],\n",
    " [0.9893535 ],\n",
    " [0.99133193]], dtype=float32)\n",
    "model.compile(\n",
    "optimizer = \"adam\",\n",
    "loss = \"binary_crossentropy\",\n",
    "metrics = [\"accuracy\"]\n",
    ")\n",
    "results = model.fit(\n",
    "X_train, y_train,\n",
    "epochs= 2,\n",
    "batch_size = 500,\n",
    "validation_data = (X_test, y_test),\n",
    "callbacks=[callback]\n",
    ") \n",
    "print(np.mean(results.history[\"val_accuracy\"])) \n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aim: Convolutional Neural Network (CNN)\n",
    "In [1]:\n",
    "In [2]:\n",
    "In [3]:\n",
    "In [4]:\n",
    "In [5]:\n",
    "In [6]:\n",
    "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-dat\n",
    "asets/train-labels-idx1-ubyte.gz (https://storage.googleapis.com/tensorflow/\n",
    "tf-keras-datasets/train-labels-idx1-ubyte.gz)\n",
    "29515/29515 [==============================] - 0s 2us/step\n",
    "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-dat\n",
    "asets/train-images-idx3-ubyte.gz (https://storage.googleapis.com/tensorflow/\n",
    "tf-keras-datasets/train-images-idx3-ubyte.gz)\n",
    "26421880/26421880 [==============================] - 176s 7us/step\n",
    "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-dat\n",
    "asets/t10k-labels-idx1-ubyte.gz (https://storage.googleapis.com/tensorflow/t\n",
    "f-keras-datasets/t10k-labels-idx1-ubyte.gz)\n",
    "5148/5148 [==============================] - 0s 0s/step\n",
    "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-dat\n",
    "asets/t10k-images-idx3-ubyte.gz (https://storage.googleapis.com/tensorflow/t\n",
    "f-keras-datasets/t10k-images-idx3-ubyte.gz)\n",
    "4422102/4422102 [==============================] - 8s 2us/step\n",
    "Out[3]: (60000, 28, 28, 1)\n",
    "Out[4]: (10000, 28, 28, 1)\n",
    "Out[5]: (60000,)\n",
    "Out[6]: (10000,)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "x_train.shape\n",
    "x_test.shape\n",
    "y_train.shape\n",
    "y_test.shape\n",
    "In [7]: model = keras.Sequential([\n",
    " keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    " keras.layers.MaxPooling2D((2,2)),\n",
    " keras.layers.Dropout(0.25),\n",
    " keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    " keras.layers.MaxPooling2D((2,2)),\n",
    " keras.layers.Dropout(0.25),\n",
    " \n",
    " keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    " keras.layers.Flatten(),\n",
    " keras.layers.Dense(128, activation='relu'),\n",
    " keras.layers.Dropout(0.25),\n",
    " keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "In [8]:\n",
    "In [10]:\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type) Output Shape Param # \n",
    "=================================================================\n",
    "conv2d (Conv2D) (None, 26, 26, 32) 320 \n",
    " \n",
    "max_pooling2d (MaxPooling2D (None, 13, 13, 32) 0 \n",
    ") \n",
    "dropout (Dropout) (None, 13, 13, 32) 0 \n",
    "conv2d_1 (Conv2D) (None, 11, 11, 64) 18496 \n",
    "max_pooling2d_1 (MaxPooling (None, 5, 5, 64) 0 \n",
    "2D) \n",
    "dropout_1 (Dropout) (None, 5, 5, 64) 0 \n",
    "conv2d_2 (Conv2D) (None, 3, 3, 128) 73856 \n",
    "flatten (Flatten) (None, 1152) 0 \n",
    "dense (Dense) (None, 128) 147584 \n",
    "dropout_2 (Dropout) (None, 128) 0 \n",
    "dense_1 (Dense) (None, 10) 1290 \n",
    "=================================================================\n",
    "Total params: 241,546\n",
    "Trainable params: 241,546\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Epoch 1/3\n",
    "1875/1875 [==============================] - 46s 24ms/step - loss: 0.4153 -\n",
    "accuracy: 0.8474 - val_loss: 0.3430 - val_accuracy: 0.8802\n",
    "Epoch 2/3\n",
    "1875/1875 [==============================] - 46s 24ms/step - loss: 0.3342 -\n",
    "accuracy: 0.8773 - val_loss: 0.3052 - val_accuracy: 0.8883\n",
    "Epoch 3/3\n",
    "1875/1875 [==============================] - 45s 24ms/step - loss: 0.3038 -\n",
    "accuracy: 0.8875 - val_loss: 0.2720 - val_accuracy: 0.8999\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metric\n",
    "history = model.fit(x_train, y_train, epochs=3, validation_data=(x_test, y_te\n",
    "In [11]:\n",
    "313/313 [==============================] - 2s 7ms/step - loss: 0.2720 - accu\n",
    "racy: 0.8999\n",
    "Test accuracy: 0.8999000191688538\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
